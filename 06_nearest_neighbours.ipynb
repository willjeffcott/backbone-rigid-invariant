{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","import os\n","\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NvVbEJanrotf","executionInfo":{"status":"ok","timestamp":1768644235975,"user_tz":0,"elapsed":16946,"user":{"displayName":"Will Jeffcott","userId":"07148715354986978812"}},"outputId":"d8094ccc-5dc8-4320-d1e4-8187d0a60ecc"},"id":"NvVbEJanrotf","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":2,"id":"2f75d542","metadata":{"id":"2f75d542","executionInfo":{"status":"ok","timestamp":1768644239047,"user_tz":0,"elapsed":3069,"user":{"displayName":"Will Jeffcott","userId":"07148715354986978812"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import tqdm\n","from scipy.spatial import KDTree\n","import ast\n","import seaborn as sns\n","import time\n","import os\n","from multiprocessing import Pool\n","import requests\n","import ast\n","from matplotlib.colors import LogNorm\n","import pickle\n","import scipy.sparse as sp\n","from scipy.signal import convolve2d\n","import gc"]},{"cell_type":"code","source":["os.chdir('/content/drive/MyDrive/BRI Analysis')"],"metadata":{"id":"z8QKEwUWr5yq","executionInfo":{"status":"ok","timestamp":1768644239557,"user_tz":0,"elapsed":479,"user":{"displayName":"Will Jeffcott","userId":"07148715354986978812"}}},"id":"z8QKEwUWr5yq","execution_count":3,"outputs":[]},{"cell_type":"code","source":["restrict_suffix = \"\""],"metadata":{"id":"S5NJDDIyWOB1","executionInfo":{"status":"ok","timestamp":1768644307888,"user_tz":0,"elapsed":7,"user":{"displayName":"Will Jeffcott","userId":"07148715354986978812"}}},"id":"S5NJDDIyWOB1","execution_count":6,"outputs":[]},{"cell_type":"markdown","id":"37f798c0","metadata":{"id":"37f798c0"},"source":["### Now do some plotting"]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import tqdm\n","import gc\n","\n","# @title 1. Generate Mean Invariants with Batch Numbers\n","inv_dir = \"./data/bri_computations\"\n","output_file = \"./data/PDB727K_mean_invariants_with_batch.csv\"\n","\n","if os.path.exists(output_file):\n","    os.remove(output_file)\n","\n","# Get files and sort them to ensure deterministic order (optional but good practice)\n","files = sorted([f for f in os.listdir(inv_dir) if f.endswith('.parquet')])\n","\n","for i, filename in enumerate(tqdm.tqdm(files, desc=\"Processing Batches\")):\n","    try:\n","        # Extract batch number from filename \"batch_123.parquet\"\n","        # Adjust split logic if your naming convention differs\n","        try:\n","            batch_num = int(filename.split('_')[1].split('.')[0])\n","        except (IndexError, ValueError):\n","            # Fallback if filename is weird, though user stated \"batch_i.parquet\"\n","            print(f\"Warning: Could not parse batch number from {filename}. assigning {i}.\")\n","            batch_num = i\n","\n","        inv_data = pd.read_parquet(os.path.join(inv_dir, filename))\n","\n","        # Calculate means\n","        mean_data = calculate_means(inv_data)\n","\n","        # --- ADD BATCH NUMBER ---\n","        mean_data['batch_number'] = batch_num\n","\n","        # Write incrementally\n","        mode = 'w' if i == 0 else 'a'\n","        header = (i == 0)\n","        mean_data.to_csv(output_file, index=False, mode=mode, header=header)\n","\n","        del inv_data, mean_data\n","        if i % 10 == 0: gc.collect()\n","\n","    except Exception as e:\n","        print(f\"Skipping {filename} due to error: {e}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sslLdMcHR6BI","executionInfo":{"status":"ok","timestamp":1768576408224,"user_tz":0,"elapsed":503805,"user":{"displayName":"Will Jeffcott","userId":"07148715354986978812"}},"outputId":"4d69d87d-7611-4167-9f85-a493d7affb8c"},"id":"sslLdMcHR6BI","execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["Processing Batches: 100%|██████████| 146/146 [08:23<00:00,  3.45s/it]\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from scipy.spatial import cKDTree\n","import os\n","import tqdm\n","import gc\n","\n","# @title 2. Mean Comparison (Partitioned by Chain Length) with Distance\n","restrict_suffix = \"\"\n","input_path = \"./data/PDB727K_mean_invariants_with_batch.csv\"\n","output_csv = f\"./data/PDB727K_mean_pairs_chebyshev_001{restrict_suffix}.csv\" # Updated filename for clarity\n","\n","bri_features = ['x(N)', 'y(N)', 'z(N)', 'x(A)', 'y(A)', 'z(A)', 'x(C)', 'y(C)', 'z(C)']\n","id_cols = ['pdb_id', 'model_id', 'chain_id', 'start_residue', 'chain_length', 'batch_number']\n","\n","# --- UPDATED THRESHOLD ---\n","radius = 0.01\n","# -------------------------\n","\n","query_batch_size = 5000\n","\n","if not os.path.exists(input_path):\n","    raise FileNotFoundError(f\"Run Step 1 first to generate {input_path}\")\n","\n","print(f\"Loading data...\")\n","mean_data_complete = pd.read_csv(input_path)\n","\n","# 1. Prepare Data\n","valid_data = mean_data_complete.dropna(subset=bri_features)\n","\n","# 2. Initialize Output\n","if os.path.exists(output_csv):\n","    os.remove(output_csv)\n","\n","# Update Header to include distance\n","header_cols = [f\"{c}_1\" for c in id_cols] + [f\"{c}_2\" for c in id_cols] + ['chebyshev_dist']\n","pd.DataFrame(columns=header_cols).to_csv(output_csv, index=False)\n","\n","# 3. Process by Chain Length\n","grouped = valid_data.groupby('chain_length')\n","\n","print(f\"Processing {len(grouped)} unique chain lengths...\")\n","total_pairs = 0\n","\n","for length, group_df in tqdm.tqdm(grouped, desc=\"Processing Length Groups\"):\n","\n","    if len(group_df) < 2:\n","        continue\n","\n","    group_df = group_df.reset_index(drop=True)\n","    points = group_df[bri_features].values\n","\n","    tree = cKDTree(points)\n","    group_pairs = []\n","\n","    for i in range(0, len(points), query_batch_size):\n","        batch_points = points[i : i + query_batch_size]\n","\n","        try:\n","            results = tree.query_ball_point(batch_points, r=radius, p=np.inf, workers=-1)\n","        except TypeError:\n","            results = tree.query_ball_point(batch_points, r=radius, p=np.inf)\n","\n","        for local_idx, neighbors in enumerate(results):\n","            if len(neighbors) < 2: continue\n","\n","            global_idx_1 = i + local_idx\n","\n","            # Filter (j > i)\n","            valid_neighbors = [n for n in neighbors if n > global_idx_1]\n","\n","            if valid_neighbors:\n","                group_pairs.extend([(global_idx_1, n) for n in valid_neighbors])\n","\n","    if group_pairs:\n","        total_pairs += len(group_pairs)\n","\n","        pairs_arr = np.array(group_pairs)\n","\n","        # 4. Calculate Distance Vectorized\n","        p1 = points[pairs_arr[:, 0]]\n","        p2 = points[pairs_arr[:, 1]]\n","\n","        dists = np.max(np.abs(p1 - p2), axis=1)\n","\n","        # Map indices to identifiers\n","        df_1 = group_df.iloc[pairs_arr[:, 0]][id_cols].reset_index(drop=True)\n","        df_2 = group_df.iloc[pairs_arr[:, 1]][id_cols].reset_index(drop=True)\n","\n","        df_1.columns = [f\"{c}_1\" for c in id_cols]\n","        df_2.columns = [f\"{c}_2\" for c in id_cols]\n","\n","        # Create Distance DataFrame\n","        dist_df = pd.DataFrame({'chebyshev_dist': dists})\n","\n","        # Concatenate Identifiers + Distance\n","        output_chunk = pd.concat([df_1, df_2, dist_df], axis=1)\n","\n","        output_chunk.to_csv(output_csv, mode='a', header=False, index=False)\n","\n","    del tree, points, group_df, group_pairs\n","    if total_pairs % 10000 == 0:\n","        gc.collect()\n","\n","print(f\"Done. Found {total_pairs} pairs across all lengths.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A5txPdHBVyxg","executionInfo":{"status":"ok","timestamp":1768645073967,"user_tz":0,"elapsed":19691,"user":{"displayName":"Will Jeffcott","userId":"07148715354986978812"}},"outputId":"204985a7-ef0c-4747-c7b1-314e2a0f63c7"},"id":"A5txPdHBVyxg","execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading data...\n","Processing 1247 unique chain lengths...\n"]},{"output_type":"stream","name":"stderr","text":["Processing Length Groups: 100%|██████████| 1247/1247 [00:17<00:00, 69.59it/s] "]},{"output_type":"stream","name":"stdout","text":["Done. Found 3247407 pairs across all lengths.\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import os\n","import tqdm\n","import gc\n","\n","# @title 3. Full Comparison (Memory Optimized: Filter-on-Load)\n","# ==============================================================================\n","# Configuration\n","# ==============================================================================\n","restrict_suffix = \"\"\n","pairs_file = f\"./data/PDB727K_mean_pairs_chebyshev_001{restrict_suffix}.csv\"\n","parquet_dir = \"./data/bri_computations\"\n","output_full_diff_file = f\"./data/PDB727K_full_comparison_results_001_seq{restrict_suffix}.csv\"\n","\n","full_dist_threshold = 0.01\n","\n","# Columns to load from Parquet\n","id_cols = ['pdb_id', 'model_id', 'chain_id', 'start_residue', 'chain_length']\n","bri_cols = ['x(N)', 'y(N)', 'z(N)', 'x(A)', 'y(A)', 'z(A)', 'x(C)', 'y(C)', 'z(C)']\n","seq_col = 'residue_label'\n","\n","load_columns = list(set(id_cols + bri_cols + [seq_col]))\n","\n","# ==============================================================================\n","# 1. Identify \"Relevant Chains\"\n","# ==============================================================================\n","if not os.path.exists(pairs_file):\n","    raise FileNotFoundError(\"Run Step 2 first.\")\n","\n","print(\"Loading pairs to identify relevant chains...\")\n","pairs_df = pd.read_csv(pairs_file)\n","\n","if len(pairs_df) == 0:\n","    print(\"No pairs found.\")\n","    exit()\n","\n","# Extract unique keys (Chain 1 and Chain 2) needed for analysis\n","# We use a set of tuples for O(1) lookup: (pdb_id, model_id, chain_id, start_residue, chain_length)\n","print(\"Building set of required chains...\")\n","keys_1 = list(zip(pairs_df['pdb_id_1'], pairs_df['model_id_1'], pairs_df['chain_id_1'], pairs_df['start_residue_1'], pairs_df['chain_length_1']))\n","keys_2 = list(zip(pairs_df['pdb_id_2'], pairs_df['model_id_2'], pairs_df['chain_id_2'], pairs_df['start_residue_2'], pairs_df['chain_length_2']))\n","\n","required_keys = set(keys_1) | set(keys_2)\n","\n","print(f\"Total unique chains to load: {len(required_keys)}\")\n","\n","# ==============================================================================\n","# 2. Load and Filter Data (One Pass over Files)\n","# ==============================================================================\n","# Store data as: chain_data[key] = {'mat': np.array, 'seq': str}\n","chain_data_store = {}\n","\n","# Get list of batch files\n","batch_files = sorted([f for f in os.listdir(parquet_dir) if f.endswith('.parquet')])\n","\n","print(f\"Scanning {len(batch_files)} batch files...\")\n","\n","for f in tqdm.tqdm(batch_files, desc=\"Loading Data\"):\n","    try:\n","        path = os.path.join(parquet_dir, f)\n","\n","        # Load batch (only relevant columns)\n","        df = pd.read_parquet(path, columns=load_columns)\n","\n","        # Create a tuple key column for filtering\n","        # Note: Vectorized zip is faster than apply\n","        # We ensure types match the pairs_df types (usually int/str)\n","        current_keys = list(zip(df['pdb_id'], df['model_id'], df['chain_id'], df['start_residue'], df['chain_length']))\n","\n","        # Filter: keep rows where the key is in our required set\n","        # Using a boolean mask with map/set is fast\n","        mask = [k in required_keys for k in current_keys]\n","\n","        if not any(mask):\n","            continue # Nothing useful in this batch\n","\n","        filtered_df = df[mask].copy()\n","\n","        # Group by chain to extract Matrix and Sequence\n","        # We groupby the full key\n","        grouped = filtered_df.groupby(id_cols)\n","\n","        for key, group in grouped:\n","            # key is the tuple (pdb, model, chain, start, length)\n","\n","            # Extract Matrix\n","            mat = group[bri_cols].to_numpy()\n","\n","            # Extract Sequence\n","            labels = group[seq_col]\n","            if len(labels) > 0 and isinstance(labels.iloc[0], str):\n","                # Standard case: sequence of characters\n","                seq = \"\".join(labels)\n","            else:\n","                seq = \"\"\n","\n","            chain_data_store[key] = {'mat': mat, 'seq': seq}\n","\n","        del df, filtered_df, mask, current_keys\n","        # gc.collect() # Optional here, usually not needed per file if filtered aggressively\n","\n","    except Exception as e:\n","        print(f\"Error reading {f}: {e}\")\n","\n","print(f\"Successfully loaded {len(chain_data_store)} chains into memory.\")\n","\n","# ==============================================================================\n","# 3. Compute Distances\n","# ==============================================================================\n","print(\"Computing pairwise comparisons...\")\n","\n","results_list = []\n","\n","# Iterate through pairs and lookup data from memory\n","for idx, row in tqdm.tqdm(pairs_df.iterrows(), total=len(pairs_df), desc=\"Comparing\"):\n","\n","    key1 = (row['pdb_id_1'], row['model_id_1'], row['chain_id_1'], row['start_residue_1'], row['chain_length_1'])\n","    key2 = (row['pdb_id_2'], row['model_id_2'], row['chain_id_2'], row['start_residue_2'], row['chain_length_2'])\n","\n","    # Retrieve data\n","    if key1 not in chain_data_store or key2 not in chain_data_store:\n","        # Should not happen if logic is correct, but safe to skip\n","        continue\n","\n","    data1 = chain_data_store[key1]\n","    data2 = chain_data_store[key2]\n","\n","    mat1 = data1['mat']\n","    mat2 = data2['mat']\n","\n","    # Check length compatibility (should match from Step 2)\n","    min_len = min(len(mat1), len(mat2))\n","\n","    # Compute Distance\n","    dist = np.max(np.abs(mat1[:min_len] - mat2[:min_len]))\n","\n","    # Check Threshold\n","    if dist <= full_dist_threshold:\n","        seq1 = data1['seq']\n","        seq2 = data2['seq']\n","\n","        res_row = row.to_dict()\n","        res_row['full_chebyshev_dist'] = dist\n","        res_row['sequence_1'] = seq1\n","        res_row['sequence_2'] = seq2\n","        res_row['sequences_identical'] = 1 if seq1 == seq2 else 0\n","\n","        results_list.append(res_row)\n","\n","# ==============================================================================\n","# 4. Save Results\n","# ==============================================================================\n","if results_list:\n","    final_df = pd.DataFrame(results_list)\n","    final_df.to_csv(output_full_diff_file, index=False)\n","    print(f\"Saved {len(final_df)} passing pairs to {output_full_diff_file}\")\n","else:\n","    print(\"No pairs passed the full distance threshold.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d7kipRCxfsXT","executionInfo":{"status":"ok","timestamp":1768646566413,"user_tz":0,"elapsed":302502,"user":{"displayName":"Will Jeffcott","userId":"07148715354986978812"}},"outputId":"3cc7149b-8460-4120-cee8-364305e3de8c"},"id":"d7kipRCxfsXT","execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading pairs to identify relevant chains...\n","Building set of required chains...\n","Total unique chains to load: 288643\n","Scanning 146 batch files...\n"]},{"output_type":"stream","name":"stderr","text":["Loading Data: 100%|██████████| 146/146 [03:01<00:00,  1.24s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Successfully loaded 287499 chains into memory.\n","Computing pairwise comparisons...\n"]},{"output_type":"stream","name":"stderr","text":["Comparing: 100%|██████████| 3247407/3247407 [01:49<00:00, 29759.81it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Saved 832774 passing pairs to ./data/PDB727K_full_comparison_results_001_seq.csv\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import os\n","\n","# Configuration\n","restrict_suffix = \"\"\n","input_file = f\"./data/PDB727K_full_comparison_results_001_seq{restrict_suffix}.csv\"\n","output_dir = './plotting/nearest_neighbours_001A'\n","\n","# 1. Create Output Directory\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)\n","\n","# Check if file exists\n","if os.path.exists(input_file):\n","    print(\"Loading data for plotting...\")\n","    df = pd.read_csv(input_file)\n","\n","    # Filter data\n","    diff_seq_data = df[df['sequences_identical'] == 0]\n","    same_seq_data = df[df['sequences_identical'] == 1]\n","\n","    # Common Plot Settings\n","    x_label = r'$L_{\\infty}$ distance on pairs of BRI, Angstroms'\n","    y_label = 'Pairs of close chains'\n","    bins_range = (0, 0.01)\n","    bin_width = 0.001\n","\n","    # Helper function to generate plots efficiently\n","    def generate_histogram(data, color, filename_suffix, log_scale=False):\n","        plt.figure(figsize=(10, 6))\n","        sns.set_style(\"whitegrid\")\n","        sns.set(font_scale=1.2)\n","\n","        # Plot (Note: edgecolor removed to drop black border)\n","        sns.histplot(\n","            data=data,\n","            x='full_chebyshev_dist',\n","            binwidth=bin_width,\n","            binrange=bins_range,\n","            color=color,\n","            element=\"bars\",\n","            linewidth=0  # Explicitly ensure no border\n","        )\n","\n","        if log_scale:\n","            plt.yscale('log')\n","            filename_suffix += \"_log\"\n","\n","        plt.xlabel(x_label)\n","        plt.ylabel(y_label)\n","        # No Title\n","\n","        plt.tight_layout()\n","\n","        # Construct filename\n","        filename = f'PDB727K_pairwise_BRI_comparisons_{filename_suffix}.png'\n","        save_path = os.path.join(output_dir, filename)\n","\n","        plt.savefig(save_path)\n","        plt.close()\n","\n","        print(f\"Saved: {save_path}\")\n","\n","    # --- Generate the 4 Plots ---\n","\n","    # 1. Linear Scale\n","    generate_histogram(diff_seq_data, 'orange', 'different_seq', log_scale=False)\n","    generate_histogram(same_seq_data, 'cornflowerblue', 'identical_seq', log_scale=False)\n","\n","    # 2. Log Scale\n","    generate_histogram(diff_seq_data, 'orange', 'different_seq', log_scale=True)\n","    generate_histogram(same_seq_data, 'cornflowerblue', 'identical_seq', log_scale=True)\n","\n","else:\n","    print(f\"Input file not found: {input_file}\")\n","    print(\"Please ensure you have run the 'Full Comparison' step to generate the results CSV.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l1X3F_KsjbPH","executionInfo":{"status":"ok","timestamp":1768647428523,"user_tz":0,"elapsed":2761,"user":{"displayName":"Will Jeffcott","userId":"07148715354986978812"}},"outputId":"5d8a1e59-a931-42fa-f60b-7bf13986667e"},"id":"l1X3F_KsjbPH","execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading data for plotting...\n","Saved: ./plotting/nearest_neighbours_001A/PDB727K_pairwise_BRI_comparisons_different_seq.png\n","Saved: ./plotting/nearest_neighbours_001A/PDB727K_pairwise_BRI_comparisons_identical_seq.png\n","Saved: ./plotting/nearest_neighbours_001A/PDB727K_pairwise_BRI_comparisons_different_seq_log.png\n","Saved: ./plotting/nearest_neighbours_001A/PDB727K_pairwise_BRI_comparisons_identical_seq_log.png\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"V6E1"},"accelerator":"TPU"},"nbformat":4,"nbformat_minor":5}